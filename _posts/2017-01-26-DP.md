---
layout: post
title: "Dynamic Programming"
categories: general
author: Octavio Ruiz Lacedelli
excerpt_separator: <!--more-->
comments: true
---

This week we read and discussed two papers in the area of dynamic programming: "Interpretable policies for dynamic product recommendations" by M. Petrik and R. Luss, and "Why most decisions are easy in Tetris &mdash; and perhaps in other sequential decision problems, as well" by Simsek et al. The main objective of the papers is to provide a systematic procedure to reduce the size of the policy space: the first one by constraining to "interpretable" policies, which are suboptimal solutions to the original problem but are easy to implement and understand, and the latter by removing actions that are dominated by other actions, effectively reducing the search space and providing fast algorithms for control. In the following sections, we describe the problem that each paper discusses, as well as the tools employed to tackle them and the experiments they performed. 

<!--more-->

_The figures below are taken from the aforementioned papers._

# Interpretable Policies for Dynamic Product Recommendations

The idea of computing an optimal policy that is interpretable is discussed in this paper. The motivation is that in many applications of Markov decision processes (MDP), optimal policies can be rather complex, making them hard to implement and interpret. The case for simpler implementation and interpretation of policies is evident in critical domains, such as medicine, where decision makers, rather than trusting a black box, prefer to work with simpler policies with the possibility to verify that the policy provides appropriate solutions, debug inappropriate assumptions or parameters, and apply the resulting policy with ease and flexibility.

With this goal in mind, the paper focuses on a simple model of policy interpretability: the policy must prescribe the same action for a well defined subset of states. The reasoning behind this is that in large problems, with thousands or millions of states, the number of rules to be learned is also very large, and calculating it can be very expensive. By constraining policies to interpretable policies, the number of rules is reduced to the state partition, which could decrease the complexity of the problem in many orders of magnitude.

In the following sections, we follow the order of exposition of the paper by first formulating the interpretable MDP as a dynamic program (DP), followed by a NLP formulation of the problem &mdash; a natural extension to the LP formulation of an MDP &mdash; and NP hardness of the problem is proved. Using the McCormick inequalities, a Mixed Integer Linear Program relaxation of the problem is developed that gives an exact solution when the problem is restricted to finding deterministic policies.  

## Notation and setup of the Interpretable MDP

Denote by $$\Delta^d$$ the $$d$$-dimensional simplex. An interpretable MDP is defined as a tuple $$(\mathcal{S}, \mathcal{A}, p_0,P, r , \mathcal{O}, \theta)$$, where $$\mathcal{S}$$ is a finite set of states, $$\mathcal{A}$$ is a finite set of actions, $$p_0 \in \Delta^{\|\mathcal{S}\|}$$ is the initial state distribution, and transitions are made for each $$a \in \mathcal{A}$$ according to the probability $$P_a \in \mathbb{R}^{\|\mathcal{S}\| \times \|\mathcal{S}\|}$$, where each row $$P_a(s,\cdot)$$ lies in the simplex  $$\Delta^{\|\mathcal{S}\|}$$ for all $$s \in \mathcal{S}$$. Finally, $$r_a \in \mathbb{R}^{\|\mathcal{S}\|}$$ denotes the reward vector for each action.

Interpretability is captured by state aggregation: augmenting the model with a set of observation $$\mathcal{O}$$ and a function $$\theta$$ which defines a partition of $$\mathcal{S}$$ by mapping states to observations. In this sense, if we define the set of deterministic stationary policies by $$\Pi_R : \{ \mathcal{S} \to \mathcal{A} \}$$, then the set of interpretable policies is the subset of $$\Pi_R$$ that satisfies

$$
\Pi_I = \{ \pi \in \Pi_R : \theta(s_1) = \theta(s_2) \Longrightarrow \pi(s_1) = \pi(s_2) \}
$$

The objective of the decision maker (DM) is then to compute a policy that maximizes the infinite horizon $$\gamma$$-discounted return $$\rho(\pi)$$, by solving the optimization problem

$$
\max_{\pi \in \Pi_I} \rho(\pi) := \max_{\pi \in \Pi_I} \sum_{t=0}^\infty \gamma^t p_0^T P_{\pi}^t r_{\pi}
$$


## Interpretable MDP formulations

A standard way to formulate an unconstrained MDP is to set it up as an LP by

$$
\max_{u} \qquad \sum_{s,a} u(s,a)r(s,a) 
$$

$$
\text{s.t.} \qquad \sum_{s,a} u(s,a) = p_0(s) + \sum_{s',a} \gamma P_a(s,s')u(s',a) \quad \forall s \in \mathcal{S}
$$

$$
\qquad \qquad u(s,a) \geq 0 \quad \forall s \in \mathcal{S}, a \in \mathcal{A}
$$

where $$u \in \mathbb{R}^{\|\mathcal{S}\| \times \|\mathcal{A}\|}$$ represents the policy we are trying to learn, the first constraint corresponds to the Bellman equation for each state. To adapt (\ref{eq:mdp1}) with the interpretability constrains, a new optimization variable is introduced, $$\psi \in \mathbb{R}^{\|\mathcal{O}\| \times \|\mathcal{A}\|}$$, which represents the interpretable policy as a function of the observations, instead of the states. The new NLP formulation is given by



### References

[1] Marek Petrik and Ronny Luss. Interpretable policies for dynamic product recommenda- tions. In Proc. Conf. Uncertainty Artif. Intell, page 74, 2016.

[2] Ozgur Simsek, Simon Algorta, and Amit Kothiyal. Why most decisions are easy in tetris—and perhaps in other sequential decision problems, as well. In Proceedings of The 33rd International Conference on Machine Learning, pages 1757–1765, 2016.
