{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan for the October 6th, 2016 Columbia Advanced Machine Learning seminar series ###\n",
    "\n",
    "\n",
    "\n",
    "** Outline **\n",
    "* Generative Adversarial Networks [1]\n",
    " * Introduction: Goal, learn (and sample) from a distribution.  \n",
    " * Previous attempts (e.g. Restricted Boltzman machine)\n",
    " * Relation to noise contrastive estimation [7]\n",
    " * Algorithm \n",
    " \n",
    "* f Divergences as a more general framework [2]. \n",
    " * What is a f divergence [9]\n",
    " * Fenchel conjugate [10], derivation of a lower bound [11,12]\n",
    " * De-mystification of GANs\n",
    "\n",
    "* Alternative methods: Maximum Mean Discrepancy Optimization [6,7] based in the Kernel Two sample test from [13]\n",
    "\n",
    "* Further discussion [3,4,5]\n",
    "\n",
    " \n",
    " \n",
    "** References **\n",
    "\n",
    "Main papers: \n",
    "* [1][Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661v1.pdf) Goodfellow et al, NIPS 2014\n",
    "\n",
    "* [2][f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization](https://arxiv.org/pdf/1606.00709v1.pdf) Nowozin et al, NIPS 2016\n",
    "\n",
    "Additional papers\n",
    "\n",
    "* [3][Improved Techniques for Training GANs](https://arxiv.org/pdf/1606.03498v1.pdf) Salimans et al, 2016, ArXiv\n",
    "* [4][On Distinguishability Criteria for Estimating Generative Models](https://arxiv.org/pdf/1412.6515.pdf) Goodfellow, 2015, ICLR Workshop\n",
    "* [5][Adversarially Learned Inference](https://arxiv.org/pdf/1606.00704v1.pdf) Dumolin et al, 2016, ArXiv\n",
    "* [6][Training generative neural networks via Maximum Mean Discrepancy optimization](https://arxiv.org/pdf/1505.03906v1.pdf) Dziugaite et al, UAI 2015\n",
    "* [7][Generative moment matching networks](https://arxiv.org/pdf/1502.02761v1.pdf) Li et al, ICML 2015\n",
    "\n",
    "\n",
    "Related ideas from the 'oldies'\n",
    "* [8][Noise-contrastive estimation: A new estimation principle for unnormalized statistical models](http://www.jmlr.org/proceedings/papers/v9/gutmann10a/gutmann10a.pdf) Gutmann and Hyv√§rinen, ICML 2010\n",
    "* [9][A general class of coefficients of divergence of one distribution from another.\n",
    "JRS](http://www.zabaras.com/Courses/BayesianComputing/Papers/fetch.pdf) Ali and Silvey, Journal of the Royal Statistical Society. Series B (Methodological) (1966): 131-142.\n",
    "* [10] [Convex Analysis](http://www.convexoptimization.com/TOOLS/ConvexAnalysis.pdf) R. Tyrell Rockafellar Princeton University Press.\n",
    "* [11] [Estimating divergence functionals and the likelihood ratio\n",
    "by convex risk minimization](http://papers.nips.cc/paper/3193-estimating-divergence-functionals-and-the-likelihood-ratio-by-penalized-convex-risk-minimization.pdf) Nguyen et al, 2008 NIPS\n",
    "* [12] [Random Variables, Monotone Relations and Convex Analysis](http://www.math.washington.edu/~rtr/papers/rtr226-Relations.pdf) Rockafellar and Royset, 2013\n",
    "* [13] [A Kernel Two-Sample Test](http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf) Gretton et al, Journal of Machine Learning Research (2012)\n",
    "\n",
    "Upcoming NIPS workshop\n",
    "* [Here](https://sites.google.com/site/nips2016adversarial/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
